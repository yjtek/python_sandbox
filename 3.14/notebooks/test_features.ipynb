{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27926238",
   "metadata": {},
   "source": [
    "### Template strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3d623",
   "metadata": {},
   "source": [
    "- Python 3.14 introduces t-strings, which you can treat as an extension of the usual f-string\n",
    "\n",
    "- You can see that a tstring returns a `Template` type, while an fstring just returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81aed5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(string.templatelib.Template, str)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'yj'\n",
    "tstring = t\"hello {name}\"\n",
    "fstring = f\"hello {name}\"\n",
    "\n",
    "type(tstring), type(fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cba9c",
   "metadata": {},
   "source": [
    "- The great thing about the `Template` is that you can sanitize inputs very easily, which avoids issues like SQL injection\n",
    "\n",
    "- Since th return value is a `Template`, you can easily cast it to anything else. For example, can return a `Prompt` in Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8935d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello YJ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string.templatelib import Interpolation\n",
    "\n",
    "amended_string=[]\n",
    "for element in tstring:\n",
    "    if isinstance(element, Interpolation):\n",
    "        amended_string.append(str(element.value).upper())\n",
    "    else:\n",
    "        amended_string.append(element)\n",
    "\n",
    "''.join(amended_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5ef53",
   "metadata": {},
   "source": [
    "### Deferred Evaluation of Annocation, and `annotationlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45735b17",
   "metadata": {},
   "source": [
    "- In previous iterations of Python, type hints are evaluated in order at runtime. So if you had functions with some class defined in the signature, with the class defined later in your code, you end up needing to use strings as the type annotation instead, or you get a runtime error\n",
    "\n",
    "- Now, type evaluation is deferred, so you no longer get an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a67d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "## No longer get runtime errors for Input1 and Output1\n",
    "def somefunc(input1: Input1) -> Output1:\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Input1:\n",
    "    attr1: str\n",
    "    attr2: float\n",
    "\n",
    "@dataclass\n",
    "class Output1:\n",
    "    output1: float\n",
    "    output2: list[float]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b6e43",
   "metadata": {},
   "source": [
    "### Multiple Interpreters + InterpreterPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d006821",
   "metadata": {},
   "source": [
    "- Previously, multiprocessing in python requires spawning a bunch of different processes\n",
    "\n",
    "- While each process has their own GIL, making parallel processing feasible, this is problematic because different processes do not share memory. So if you need to share data, SERDE is needed\n",
    "\n",
    "- In 3.14, multiprocessing is now enabled within the same process via sub-interpreters. Shared data is possible, but it also means that if the main interpreter crashes, the subinterpreters also dies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e6f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from interpreter\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "from concurrent import interpreters\n",
    "\n",
    "interpreter = interpreters.create()\n",
    "interpreter.exec('''print(\"hello from interpreter\")''')\n",
    "\n",
    "def square(n):\n",
    "    return n*n\n",
    "\n",
    "print(interpreter.call(square, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c996a750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.928771208011312 24.111683625000296 20.142291500000283\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import InterpreterPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import math\n",
    "\n",
    "def compute_factorial_concurrent():\n",
    "    start=time.perf_counter()\n",
    "    res = []\n",
    "\n",
    "    with InterpreterPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(math.factorial, i) for i in range(10000,15000)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            res.append(future.result())\n",
    "\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "def compute_factorial_threaded():\n",
    "    start=time.perf_counter()\n",
    "    res = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(math.factorial, i) for i in range(10000,15000)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            res.append(future.result())\n",
    "\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "def compute_factorial_seqeuntial():\n",
    "    start=time.perf_counter()\n",
    "    res = [math.factorial(i) for i in range(10000, 15000)]\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "\n",
    "ctime, cres = compute_factorial_concurrent()\n",
    "ttime, tres = compute_factorial_threaded()\n",
    "stime, sres = compute_factorial_seqeuntial()\n",
    "\n",
    "print(ctime, ttime, stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybrid Approach: Subinterpreters + Multithreading\n",
    "\n",
    "# Combining subinterpreters for CPU-bound tasks with threading for I/O-bound tasks\n",
    "from concurrent.futures import InterpreterPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import threading\n",
    "from concurrent import interpreters\n",
    "\n",
    "def cpu_intensive_task(n):\n",
    "    \"\"\"CPU-bound task that benefits from subinterpreters\"\"\"\n",
    "    return sum(math.factorial(i) for i in range(n, n+10))\n",
    "\n",
    "def io_intensive_task(url):\n",
    "    \"\"\"I/O-bound task that benefits from threading\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        return f\"Status: {response.status_code}, Length: {len(response.content)}\"\n",
    "    except:\n",
    "        return \"Request failed\"\n",
    "\n",
    "def hybrid_worker(cpu_tasks, io_tasks):\n",
    "    \"\"\"Worker function that runs in a subinterpreter and uses threading for I/O\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Use threading within the subinterpreter for I/O tasks\n",
    "    with ThreadPoolExecutor(max_workers=2) as thread_executor:\n",
    "        # Submit I/O tasks to threads\n",
    "        io_futures = {thread_executor.submit(io_intensive_task, url): url for url in io_tasks}\n",
    "        \n",
    "        # Process CPU tasks in the main thread of this subinterpreter\n",
    "        for task_id, n in cpu_tasks:\n",
    "            results[f\"cpu_{task_id}\"] = cpu_intensive_task(n)\n",
    "        \n",
    "        # Collect I/O results\n",
    "        for future in as_completed(io_futures):\n",
    "            url = io_futures[future]\n",
    "            results[f\"io_{url}\"] = future.result()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "cpu_workload = [(i, 100 + i*5) for i in range(10)]  # 10 CPU tasks\n",
    "io_workload = [\n",
    "    \"https://httpbin.org/delay/1\",\n",
    "    \"https://httpbin.org/delay/2\", \n",
    "    \"https://httpbin.org/delay/1\",\n",
    "    \"https://httpbin.org/delay/2\"\n",
    "]  # 4 I/O tasks\n",
    "\n",
    "print(\"Running hybrid approach...\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Split work across subinterpreters\n",
    "subinterpreter_count = 3\n",
    "cpu_chunks = [cpu_workload[i::subinterpreter_count] for i in range(subinterpreter_count)]\n",
    "io_chunks = [io_workload[i::subinterpreter_count] for i in range(subinterpreter_count)]\n",
    "\n",
    "with InterpreterPoolExecutor(max_workers=subinterpreter_count) as executor:\n",
    "    futures = [\n",
    "        executor.submit(hybrid_worker, cpu_chunks[i], io_chunks[i]) \n",
    "        for i in range(subinterpreter_count)\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    for future in as_completed(futures):\n",
    "        all_results.update(future.result())\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Hybrid execution time: {end - start:.4f} seconds\")\n",
    "print(f\"Total results collected: {len(all_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Advanced Hybrid Patterns\n",
    "\n",
    "# Pattern 1: Pipeline approach - subinterpreters for processing, threads for I/O\n",
    "def pipeline_worker(data_batch):\n",
    "    \"\"\"Process data in subinterpreter, use threads for I/O operations\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    # CPU-intensive processing in main thread of subinterpreter\n",
    "    for item in data_batch:\n",
    "        processed = math.factorial(item) + sum(range(item))\n",
    "        processed_data.append(processed)\n",
    "    \n",
    "    # I/O operations using threads within subinterpreter\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # Simulate saving processed data (I/O bound)\n",
    "        save_futures = [\n",
    "            executor.submit(simulate_save, f\"result_{i}.txt\", data) \n",
    "            for i, data in enumerate(processed_data)\n",
    "        ]\n",
    "        \n",
    "        # Wait for all saves to complete\n",
    "        save_results = [future.result() for future in as_completed(save_futures)]\n",
    "    \n",
    "    return len(processed_data), save_results\n",
    "\n",
    "def simulate_save(filename, data):\n",
    "    \"\"\"Simulate file I/O operation\"\"\"\n",
    "    time.sleep(0.1)  # Simulate I/O delay\n",
    "    return f\"Saved {filename} with data length {len(str(data))}\"\n",
    "\n",
    "# Pattern 2: Producer-consumer with subinterpreters and threads\n",
    "def producer_consumer_worker(queue_size=5):\n",
    "    \"\"\"Producer-consumer pattern using threads within subinterpreter\"\"\"\n",
    "    import queue\n",
    "    import threading\n",
    "    \n",
    "    result_queue = queue.Queue(maxsize=queue_size)\n",
    "    results = []\n",
    "    \n",
    "    def producer():\n",
    "        \"\"\"Produce CPU-intensive results\"\"\"\n",
    "        for i in range(10):\n",
    "            result = math.factorial(50 + i)\n",
    "            result_queue.put(f\"factorial_{i}: {result}\")\n",
    "    \n",
    "    def consumer():\n",
    "        \"\"\"Consume results and perform I/O operations\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                item = result_queue.get(timeout=1)\n",
    "                # Simulate I/O operation on consumed item\n",
    "                time.sleep(0.05)\n",
    "                results.append(f\"processed_{item}\")\n",
    "                result_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                break\n",
    "    \n",
    "    # Use threads within subinterpreter\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        producer_future = executor.submit(producer)\n",
    "        consumer_future = executor.submit(consumer)\n",
    "        \n",
    "        producer_future.result()\n",
    "        consumer_future.result()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Pattern 3: Nested parallelism - subinterpreters containing thread pools\n",
    "def nested_parallel_worker(work_items):\n",
    "    \"\"\"Each subinterpreter manages its own thread pool for different task types\"\"\"\n",
    "    cpu_results = []\n",
    "    io_results = []\n",
    "    \n",
    "    # Separate thread pools for different task types within subinterpreter\n",
    "    with ThreadPoolExecutor(max_workers=2) as cpu_executor, \\\n",
    "         ThreadPoolExecutor(max_workers=2) as io_executor:\n",
    "        \n",
    "        # Submit CPU tasks\n",
    "        cpu_futures = [\n",
    "            cpu_executor.submit(math.factorial, item) \n",
    "            for item in work_items if item % 2 == 0\n",
    "        ]\n",
    "        \n",
    "        # Submit I/O tasks\n",
    "        io_futures = [\n",
    "            io_executor.submit(simulate_network_call, item) \n",
    "            for item in work_items if item % 2 == 1\n",
    "        ]\n",
    "        \n",
    "        # Collect results\n",
    "        cpu_results = [future.result() for future in as_completed(cpu_futures)]\n",
    "        io_results = [future.result() for future in as_completed(io_futures)]\n",
    "    \n",
    "    return {\"cpu\": cpu_results, \"io\": io_results}\n",
    "\n",
    "def simulate_network_call(item):\n",
    "    \"\"\"Simulate network I/O\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return f\"network_result_{item}\"\n",
    "\n",
    "# Run different patterns\n",
    "print(\"\\\\n=== Pattern 1: Pipeline Approach ===\")\n",
    "start = time.perf_counter()\n",
    "with InterpreterPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [\n",
    "        executor.submit(pipeline_worker, list(range(100 + i*10, 110 + i*10)))\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    pipeline_results = [future.result() for future in as_completed(futures)]\n",
    "end = time.perf_counter()\n",
    "print(f\"Pipeline execution time: {end - start:.4f} seconds\")\n",
    "\n",
    "print(\"\\\\n=== Pattern 2: Producer-Consumer ===\")\n",
    "start = time.perf_counter()\n",
    "with InterpreterPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [executor.submit(producer_consumer_worker) for _ in range(2)]\n",
    "    pc_results = [future.result() for future in as_completed(futures)]\n",
    "end = time.perf_counter()\n",
    "print(f\"Producer-consumer execution time: {end - start:.4f} seconds\")\n",
    "\n",
    "print(\"\\\\n=== Pattern 3: Nested Parallelism ===\")\n",
    "start = time.perf_counter()\n",
    "with InterpreterPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [\n",
    "        executor.submit(nested_parallel_worker, list(range(10 + i*5, 20 + i*5)))\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    nested_results = [future.result() for future in as_completed(futures)]\n",
    "end = time.perf_counter()\n",
    "print(f\"Nested parallelism execution time: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### When to Use Hybrid Approach\n",
    "\n",
    "\"\"\"\n",
    "Key scenarios where combining subinterpreters + threading is beneficial:\n",
    "\n",
    "1. **Mixed Workloads**: When you have both CPU-intensive and I/O-intensive tasks\n",
    "2. **Resource Optimization**: Maximize CPU cores for computation while handling I/O efficiently\n",
    "3. **Pipeline Processing**: Process data in subinterpreters, save results using threads\n",
    "4. **Real-time Systems**: CPU processing in subinterpreters, async I/O in threads\n",
    "5. **Data Processing**: Transform data in subinterpreters, stream to databases via threads\n",
    "\"\"\"\n",
    "\n",
    "# Practical example: Data processing pipeline\n",
    "def data_processing_pipeline(data_chunks):\n",
    "    \"\"\"Real-world example: Process data chunks with mixed CPU/I/O operations\"\"\"\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for chunk in data_chunks:\n",
    "        # CPU-intensive: Data transformation and analysis\n",
    "        transformed_data = []\n",
    "        for item in chunk:\n",
    "            # Complex mathematical operations\n",
    "            result = {\n",
    "                'original': item,\n",
    "                'factorial': math.factorial(min(item, 20)),  # Cap to avoid overflow\n",
    "                'sum_squares': sum(i*i for i in range(item)),\n",
    "                'processed_at': time.time()\n",
    "            }\n",
    "            transformed_data.append(result)\n",
    "        \n",
    "        # I/O-intensive: Save processed data using threads\n",
    "        with ThreadPoolExecutor(max_workers=3) as io_executor:\n",
    "            save_futures = [\n",
    "                io_executor.submit(save_to_database, f\"chunk_{i}\", data)\n",
    "                for i, data in enumerate(transformed_data)\n",
    "            ]\n",
    "            \n",
    "            # Wait for all saves to complete\n",
    "            save_results = [future.result() for future in as_completed(save_futures)]\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            'chunk_size': len(transformed_data),\n",
    "            'saves_completed': len(save_results)\n",
    "        })\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "def save_to_database(key, data):\n",
    "    \"\"\"Simulate database save operation\"\"\"\n",
    "    time.sleep(0.05)  # Simulate database I/O\n",
    "    return f\"Saved {key} with {len(str(data))} characters\"\n",
    "\n",
    "# Performance comparison: Hybrid vs Pure approaches\n",
    "def benchmark_approaches():\n",
    "    \"\"\"Compare different approaches for mixed workloads\"\"\"\n",
    "    data = [list(range(100 + i*20, 120 + i*20)) for i in range(5)]\n",
    "    \n",
    "    print(\"\\\\n=== Performance Comparison ===\")\n",
    "    \n",
    "    # Approach 1: Pure subinterpreters (CPU tasks only)\n",
    "    start = time.perf_counter()\n",
    "    with InterpreterPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(lambda chunk: [math.factorial(min(x, 15)) for x in chunk], chunk) for chunk in data]\n",
    "        pure_subinterpreter_results = [future.result() for future in as_completed(futures)]\n",
    "    pure_subinterpreter_time = time.perf_counter() - start\n",
    "    \n",
    "    # Approach 2: Pure threading (limited by GIL for CPU tasks)\n",
    "    start = time.perf_counter()\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(lambda chunk: [math.factorial(min(x, 15)) for x in chunk], chunk) for chunk in data]\n",
    "        pure_threading_results = [future.result() for future in as_completed(futures)]\n",
    "    pure_threading_time = time.perf_counter() - start\n",
    "    \n",
    "    # Approach 3: Hybrid (subinterpreters + threading)\n",
    "    start = time.perf_counter()\n",
    "    with InterpreterPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(data_processing_pipeline, [chunk]) for chunk in data]\n",
    "        hybrid_results = [future.result() for future in as_completed(futures)]\n",
    "    hybrid_time = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Pure Subinterpreters: {pure_subinterpreter_time:.4f}s\")\n",
    "    print(f\"Pure Threading: {pure_threading_time:.4f}s\") \n",
    "    print(f\"Hybrid Approach: {hybrid_time:.4f}s\")\n",
    "    print(f\"\\\\nHybrid vs Pure Subinterpreters: {pure_subinterpreter_time/hybrid_time:.2f}x\")\n",
    "    print(f\"Hybrid vs Pure Threading: {pure_threading_time/hybrid_time:.2f}x\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_approaches()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ce020",
   "metadata": {},
   "source": [
    "### Free-threaded Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da422823",
   "metadata": {},
   "source": [
    "- Not only can we have sub-interpreters to do multiprocessing, 3.14 allows you to compile python without the GIL. This is known as free threaded python\n",
    "\n",
    "- There is a difference between free-threaded multithreading vs multi processing via sub-interpreters\n",
    "    - Free-threaded multithreading:\n",
    "        - All threads can run python bytecode at the same time\n",
    "        - All threads share memory\n",
    "        - BUT you are responsible for handling race conditions vs locks and mutex\n",
    "    - Multiprocessing with sub-interpreter\n",
    "        - Every subinterpreter runs in the same process, and can execute code in parallel \n",
    "        - BUT Every subinterpreter is more or less isolated, and sharing memory will require you to used some specific mechanisms (queues, shared memory etc)\n",
    "\n",
    "- If you rerun the benchmark in previous section with free threaded python, the time taken for subinterpreter and multithreading is ~the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e601e",
   "metadata": {},
   "source": [
    "### asyncio CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bfb1f",
   "metadata": {},
   "source": [
    "- You can use a CLI tool to look at asyncio tasks\n",
    "\n",
    "- Assume you have a python process running and awaiting; you can use the call below to see the asyncio task status\n",
    "    - `sudo .venv/bin/python  -m  asyncio ps 12345`\n",
    "    - `sudo .venv/bin/python  -m  asyncio pstree 12345` \n",
    "\n",
    "- The process value can be found by running `os.getpid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78837a3e",
   "metadata": {},
   "source": [
    "### Compression algorithms combined into 1 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213a9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression import zlib, bz2, lzma, gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02f816",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
