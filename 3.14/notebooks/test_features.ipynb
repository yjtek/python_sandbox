{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27926238",
   "metadata": {},
   "source": [
    "### Template strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3d623",
   "metadata": {},
   "source": [
    "- Python 3.14 introduces t-strings, which you can treat as an extension of the usual f-string\n",
    "\n",
    "- You can see that a tstring returns a `Template` type, while an fstring just returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81aed5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(string.templatelib.Template, str)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'yj'\n",
    "tstring = t\"hello {name}\"\n",
    "fstring = f\"hello {name}\"\n",
    "\n",
    "type(tstring), type(fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cba9c",
   "metadata": {},
   "source": [
    "- The great thing about the `Template` is that you can sanitize inputs very easily, which avoids issues like SQL injection\n",
    "\n",
    "- Since th return value is a `Template`, you can easily cast it to anything else. For example, can return a `Prompt` in Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8935d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello YJ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string.templatelib import Interpolation\n",
    "\n",
    "amended_string=[]\n",
    "for element in tstring:\n",
    "    if isinstance(element, Interpolation):\n",
    "        amended_string.append(str(element.value).upper())\n",
    "    else:\n",
    "        amended_string.append(element)\n",
    "\n",
    "''.join(amended_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5ef53",
   "metadata": {},
   "source": [
    "### Deferred Evaluation of Annocation, and `annotationlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45735b17",
   "metadata": {},
   "source": [
    "- In previous iterations of Python, type hints are evaluated in order at runtime. So if you had functions with some class defined in the signature, with the class defined later in your code, you end up needing to use strings as the type annotation instead, or you get a runtime error\n",
    "\n",
    "- Now, type evaluation is deferred, so you no longer get an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a67d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "## No longer get runtime errors for Input1 and Output1\n",
    "def somefunc(input1: Input1) -> Output1:\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Input1:\n",
    "    attr1: str\n",
    "    attr2: float\n",
    "\n",
    "@dataclass\n",
    "class Output1:\n",
    "    output1: float\n",
    "    output2: list[float]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b6e43",
   "metadata": {},
   "source": [
    "### Multiple Interpreters + InterpreterPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d006821",
   "metadata": {},
   "source": [
    "- Previously, multiprocessing in python requires spawning a bunch of different processes\n",
    "\n",
    "- While each process has their own GIL, making parallel processing feasible, this is problematic because different processes do not share memory. So if you need to share data, SERDE is needed\n",
    "\n",
    "- In 3.14, multiprocessing is now enabled within the same process via sub-interpreters. Shared data is possible, but it also means that if the main interpreter crashes, the subinterpreters also dies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e6f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25hello from interpreter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent import interpreters\n",
    "\n",
    "interpreter = interpreters.create()\n",
    "interpreter.exec('''print(\"hello from interpreter\")''')\n",
    "\n",
    "def square(n):\n",
    "    return n*n\n",
    "\n",
    "print(interpreter.call(square, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c996a750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.419310166995274 22.79251599998679 20.940022791997762\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import InterpreterPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import math\n",
    "\n",
    "def compute_factorial_concurrent():\n",
    "    start=time.perf_counter()\n",
    "    res = []\n",
    "\n",
    "    with InterpreterPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(math.factorial, i) for i in range(10000,15000)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            res.append(future.result())\n",
    "\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "def compute_factorial_threaded():\n",
    "    start=time.perf_counter()\n",
    "    res = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(math.factorial, i) for i in range(10000,15000)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            res.append(future.result())\n",
    "\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "def compute_factorial_seqeuntial():\n",
    "    start=time.perf_counter()\n",
    "    res = [math.factorial(i) for i in range(10000, 15000)]\n",
    "    end=time.perf_counter()\n",
    "    return end-start, res\n",
    "\n",
    "\n",
    "ctime, cres = compute_factorial_concurrent()\n",
    "ttime, tres = compute_factorial_threaded()\n",
    "stime, sres = compute_factorial_seqeuntial()\n",
    "\n",
    "print(ctime, ttime, stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ce020",
   "metadata": {},
   "source": [
    "### Free-threaded Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da422823",
   "metadata": {},
   "source": [
    "- Not only can we have sub-interpreters to do multiprocessing, 3.14 allows you to compile python without the GIL. This is known as free threaded python\n",
    "\n",
    "- There is a difference between free-threaded multithreading vs multi processing via sub-interpreters\n",
    "    - Free-threaded multithreading:\n",
    "        - All threads can run python bytecode at the same time\n",
    "        - All threads share memory\n",
    "        - BUT you are responsible for handling race conditions vs locks and mutex\n",
    "    - Multiprocessing with sub-interpreter\n",
    "        - Every subinterpreter runs in the same process, and can execute code in parallel \n",
    "        - BUT Every subinterpreter is more or less isolated, and sharing memory will require you to used some specific mechanisms (queues, shared memory etc)\n",
    "\n",
    "- If you rerun the benchmark in previous section with free threaded python, the time taken for subinterpreter and multithreading is ~the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e601e",
   "metadata": {},
   "source": [
    "### asyncio CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bfb1f",
   "metadata": {},
   "source": [
    "- You can use a CLI tool to look at asyncio tasks\n",
    "\n",
    "- Assume you have a python process running and awaiting; you can use the call below to see the asyncio task status\n",
    "    - `sudo .venv/bin/python  -m  asyncio ps 12345`\n",
    "    - `sudo .venv/bin/python  -m  asyncio pstree 12345` \n",
    "\n",
    "- The process value can be found by running `os.getpid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78837a3e",
   "metadata": {},
   "source": [
    "### Compression algorithms combined into 1 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213a9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression import zlib, bz2, lzma, gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02f816",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
